<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Emo3D: 3D Facial Expression Generation from Emotion Description</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
  <div class="container">
    <header>
      <h1>Emo3D</h1>
      <h2>Metric and Benchmarking Dataset for 3D Facial Expression<br>Generation from Emotion Description</h2>
      <div class="conference">NAACL 2025</div>
    </header>

    <div class="authors">
      <p>Mahshid Dehghani<sup>1,2</sup>, Amirahmad Shafiee<sup>1,2,*</sup>, Ali Shafiei<sup>1,2,*</sup>,<br>Neda Fallah<sup>1,2</sup>, Farahmand Alizadeh<sup>1,2</sup>, Mohammad Mehdi Gholinejad<sup>1,2</sup>,<br>Hamid Behroozi<sup>1</sup>, Jafar Habibi<sup>1</sup>, Ehsaneddin Asgari<sup>3</sup></p>
      <p class="affiliations">
        <sup>1</sup>Sharif University of Technology<br>
        <sup>2</sup>NLP & DH Lab, Computer Engineering Department, Sharif University of Technology<br>
        <sup>3</sup>Qatar Computing Research Institute, Doha, Qatar<br>
        <sup>*</sup>These authors contributed equally to this work
      </p>
    </div>

    <div class="links">
      <a href="https://aclanthology.org/2025.findings-naacl.173/" target="_blank" rel="noopener noreferrer" class="button">
        <i class="fas fa-file-alt"></i>
        <span>Paper</span>
      </a>
      <a href="https://github.com/MahshidDehghani/Emo3D.git" target="_blank" rel="noopener noreferrer" class="button">
        <i class="fab fa-github"></i>
        <span>Code</span>
      </a>
      <a href="https://huggingface.co/datasets/llm-lab/Emo3D" target="_blank" rel="noopener noreferrer" class="button">
        <i class="fas fa-database"></i>
        <span>Dataset</span>
      </a>
    </div>

    <div class="audio-section">
      <h3 class="section-title">Podcast Generated by Notebooklm</h3>
      <audio id="audioPlayer" controls>
        <source src="assets/notebooklm-podcast.wav" type="audio/wav">
        Your browser does not support the audio element.
      </audio>
      <label for="speedControl"></label>
      <select id="speedControl" onchange="changeSpeed()">
        <option value="0.5">0.5x</option>
        <option value="0.75">0.75x</option>
        <option value="1" selected>1x</option>
        <option value="1.25">1.25x</option>
        <option value="1.5">1.5x</option>
        <option value="2">2x</option>
      </select>
    </div>

    <div class="abstract">
      <h3>Abstract</h3>
      <p>3D facial emotion modeling has important applications in areas such as animation design, virtual reality, and emotional human-computer interaction (HCI). However, existing models are constrained by limited emotion classes and insufficient datasets. To address this, we introduce Emo3D, an extensive “Text-Image-Expression dataset” that spans a wide spectrum of human emotions, each paired with images and 3D blendshapes. Leveraging Large Language Models (LLMs), we generate a diverse array of textual descriptions, enabling the capture of a broad range of emotional expressions. Using this unique dataset, we perform a comprehensive evaluation of fine-tuned language-based models and vision-language models, such as Contrastive Language-Image Pretraining (CLIP), for 3D facial expression synthesis. To better assess conveyed emotions, we introduce Emo3D metric, a new evaluation metric that aligns more closely with human perception than traditional Mean Squared Error (MSE). Unlike MSE, which focuses on numerical differences, Emo3D captures emotional nuances in visual-text alignment and semantic richness. Emo3D dataset and metric hold great potential for advancing applications in animation and virtual reality.</p>
    </div>

    <div class="demo">
      <h3 class="section-title">Text-to-Expression Generation</h3>
      <div class="demo-content">
        <img src="assets/demo.gif" alt="Emo3D Demo" class="demo-gif">
      </div>
    </div>

    <div class="dataset-samples">
      <h3 class="section-title">Samples of Emo3D Dataset</h3>
      <div class="dataset-image">
        <img src="assets/data.png" alt="Emo3D Dataset Samples" class="dataset-image">
      </div>
    </div>

    <div class="citation">
      <h3 class="section-title">Bibtex</h3>
      <div class="bibtex">
        <pre>@inproceedings{dehghani-etal-2025-emo3d,
          title = "{E}mo3{D}: Metric and Benchmarking Dataset for 3{D} Facial Expression Generation from Emotion Description",
          author = "Dehghani, Mahshid  and
            Shafiee, Amirahmad  and
            Shafiei, Ali  and
            Fallah, Neda  and
            Alizadeh, Farahmand  and
            Gholinejad, Mohammad Mehdi  and
            Behroozi, Hamid  and
            Habibi, Jafar  and
            Asgari, Ehsaneddin",
          editor = "Chiruzzo, Luis  and
            Ritter, Alan  and
            Wang, Lu",
          booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
          month = apr,
          year = "2025",
          address = "Albuquerque, New Mexico",
          publisher = "Association for Computational Linguistics",
          url = "https://aclanthology.org/2025.findings-naacl.173/",
          pages = "3158--3172",
          ISBN = "979-8-89176-195-7",
          abstract = "3D facial emotion modeling has important applications in areas such as animation design, virtual reality, and emotional human-computer interaction (HCI). However, existing models are constrained by limited emotion classes and insufficient datasets. To address this, we introduce Emo3D, an extensive {\textquotedblleft}Text-Image-Expression dataset{\textquotedblright} that spans a wide spectrum of human emotions, each paired with images and 3D blendshapes. Leveraging Large Language Models (LLMs), we generate a diverse array of textual descriptions, enabling the capture of a broad range of emotional expressions. Using this unique dataset, we perform a comprehensive evaluation of fine-tuned language-based models and vision-language models, such as Contrastive Language-Image Pretraining (CLIP), for 3D facial expression synthesis. To better assess conveyed emotions, we introduce Emo3D metric, a new evaluation metric that aligns more closely with human perception than traditional Mean Squared Error (MSE). Unlike MSE, which focuses on numerical differences, Emo3D captures emotional nuances in visual-text alignment and semantic richness. Emo3D dataset and metric hold great potential for advancing applications in animation and virtual reality."
      }</pre>
      </div>
    </div>
  </div>

  <script>
    function changeSpeed() {
      const audio = document.getElementById("audioPlayer");
      const speed = document.getElementById("speedControl").value;
      audio.playbackRate = speed;
    }
  </script>
</body>
</html> 